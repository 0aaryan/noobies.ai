from clarifai.client.model import Model


from clarifai.client.model import Model

# input = "I love your product very much"

# api_key = ElevenLabs_API_KEY

# inference_params = dict(voice-id="EXAVITQu4vr4xnSDxMaL", model_id="eleven_multilingual_v2", stability= 0.5, similarity_boost= 0.5, style=0,use_speaker_boost=True, api_key = api_key)

# # Model Predict
# model_prediction = Model("https://clarifai.com/eleven-labs/audio-generation/models/speech-synthesis").predict_by_bytes(input.encode(), input_type="text", inference_params=inference_params)

# output_base64 = model_prediction.outputs[0].data.audio.base64

# with open('audio_file3.wav', 'wb') as f:
# f.write(output_base64)


class AudioAI:
    def __init__(
        self,
        model_url="https://clarifai.com/eleven-labs/audio-generation/models/speech-synthesis",
    ):
        """
        Initializes the AudioAI class.

        Args:
            user_id (str): User ID for the model (default: "openai").
            app_id (str): App ID for the model (default: "audio-e").
            model_id (str): Model ID for the model (default: "audio-e-3").
            model_version_id (str): Model version ID for the model (default: "dc9dcb6ee67543cebc0b9a025861b868").
        """
        self.model_url = model_url
        self.llm = Model(
            model_url,
        )
        self.voice_ids = {
            "male1": "2EiwWnXFnvU5JabPnv8n",
            "female1": "29vD33N1CtxCmqQRPOHJ",
        }

    def generate(
        self,
        prompt,
        inference_params={
            "voice-id": "2EiwWnXFnvU5JabPnv8n",
            "model_id": "eleven_multilingual_v2",
            "stability": 0.5,
            "similarity_boost": 0.5,
            "style": 0,
            "use_speaker_boost": True,
            "language": "hi",
        },
        output_file="audio.wav",
    ):
        """
        Generate a prediction using the given prompt template and input variables.

        Args:
            prompt_template (dict): Prompt template containing the template and input variables.
            **kwargs: Input variables as keyword arguments.

        Returns:
            dict: Prediction generated by the model.
        """
        try:
            prediction = self.llm.predict_by_bytes(
                prompt.encode(), input_type="text", inference_params=inference_params
            )
            output_base64 = prediction.outputs[0].data.audio.base64
            with open(output_file, "wb") as f:
                f.write(output_base64)
            return True
        except Exception as e:
            print(e)
            return False

    def get_voice_ids(self):
        return self.voice_ids

    def get_transcription(self, audio_path, word_timestamps=True):
        import whisper

        model = whisper.load_model("base.en")
        result = model.transcribe(audio_path, word_timestamps=word_timestamps)
        words = []
        for res in result["segments"]:
            words.extend(res["words"])
        return words


if __name__ == "__main__":
    from dotenv import load_dotenv

    load_dotenv()
    audio_ai = AudioAI()
    audio_ai.generate(
        prompt="weather is very good today. let's go to the beach",
        output_file="audio.wav",
    )

    transcript = audio_ai.get_transcription("/Data/aryanCodes3/noobies.ai/voice.mp3")
    for word in transcript:
        print(word["word"], word["start"], word["end"])
